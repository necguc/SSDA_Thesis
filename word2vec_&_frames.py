# -*- coding: utf-8 -*-
"""Word2Vec & Frames

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qXXcmDP2UJAyTcmWPx45j0RPerZ9SaYu
"""

import pandas as pd
import re
from gensim.models import Word2Vec
import nltk
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
file_path = "C:\\Users\\User\\Desktop\\Thesis\\standardized_speeches_paragraphs_with_sentencecount.xlsx"
df = pd.read_excel(file_path)

# Function to clean the text
def clean_text(text):
    # Remove apostrophes and suffixes
    text = re.sub(r"\b(\w+)'[\w\s]*\b", r'\1', text)
    # Convert to lowercase
    text = text.lower()
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    return text

# Apply the cleaning process
df['Cleaned_Paragraph'] = df['Paragraph'].apply(clean_text)

# Tokenize the sentences into words
nltk.download('punkt')
df['Tokenized_Paragraph'] = df['Cleaned_Paragraph'].apply(nltk.word_tokenize)

# Train the Word2Vec model
sentences = df['Tokenized_Paragraph'].tolist()
model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)

# Save the model
model.save("C:\\Users\\User\\Desktop\\Thesis\\word2vec.model")

# Load the model
model = Word2Vec.load("C:\\Users\\User\\Desktop\\Thesis\\word2vec.model")

# Updated and added keywords
diagnostic_keywords = ["terör", "kriz", "tehdit", "problem", "zorluk", "beka", "yolsuzluk", "lobi", "darbe", "ihanet", "provokasyon", "kargaşa", "kaos", "paralel yapı", "komplo", "yalan", "gaflet", "sömürgeciler", "dış güçler", "saldırı", "patlama", "suikast", "PKK", "FETÖ", "DEAŞ", "savaş suçları", "göçmen sorunu", "sığınmacılar", "hukuk", "yargı", "mahkeme", "sınır güvenliği", "özgürlük", "demokrasi", "harekât", "müdahale", "riskler", "oyun", "tuzak", "ekonomik kriz", "bölgesel güvenlik", "insan hakları ihlalleri"]
prognostic_keywords = ["çözüm", "reform", "plan", "strateji", "önlem", "teknoloji", "bilim", "proje", "kalkınma", "demokrasi", "sandık", "seçim", "yenilik", "vizyon", "gelişme", "hizmet", "eser", "silah teknolojisi", "askeri yenilikler", "dış müdahale", "güvenlik stratejisi", "iç güvenlik", "koruma tedbirleri", "barış çabaları", "güvenlik anlaşmaları", "altyapı geliştirme", "sanayi yatırımları", "sosyal hizmetler", "kamu yatırımları", "inşaat projeleri", "ulaşım geliştirme", "diplomatik ilişkiler", "yabancı ortaklıklar", "ekonomik kalkınma", "eğitim ve sağlık reformları"]
identity_keywords = ["biz", "halk", "millet", "vatandaş", "ulusal", "milli irade", "vatan", "egemenlik", "mağdur", "mazlum", "yoksul", "dindar", "müslüman", "türk", "güçlü türkiye", "kardeş", "birlik", "bütünlük", "beraberlik", "geçmiş", "kültürel miras", "halkın sesi", "demokrasi", "gelenekler", "örf ve adetler", "İslam", "kahramanlık", "yardımlaşma", "toplumsal destek", "yurtseverlik", "ülke sevgisi", "milli başarılar", "kahramanlık hikayeleri", "toplumsal dayanışma", "şehitlik ve gazilik"]
adversarial_keywords = ["onlar", "bunlar", "düşman", "karşıt", "rakip", "elit", "oligark", "saldıranlar", "emperyalistler", "bozguncu", "piyon", "kalemşör", "batılılar", "işbirlikçi", "içerideki odaklar", "dışarıdaki odaklar", "hain", "karşıt görüş", "muhalif", "engelleyici", "düşman devletler", "isyancılar", "ötekileştirme", "entrikalar", "oyunlar", "saldırganlar", "ambargo", "ticari kısıtlamalar", "ayrılıkçı hareketler", "bilgi savaşları", "tahrikler", "fitneler", "ekonomik yaptırımlar", "siyasi manipülasyonlar"]

# Function to calculate frame vectors
def get_average_embedding(model, keywords):
    vectors = [model.wv[word] for word in keywords if word in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

# Calculate the frame vectors
diagnostic_vector = get_average_embedding(model, diagnostic_keywords)
prognostic_vector = get_average_embedding(model, prognostic_keywords)
identity_vector = get_average_embedding(model, identity_keywords)
adversarial_vector = get_average_embedding(model, adversarial_keywords)

# Function to calculate frame scores
def calculate_frame_score(paragraph, frame_vector):
    paragraph_vectors = [model.wv[word] for word in paragraph if word in model.wv]
    if paragraph_vectors:
        paragraph_vector = np.mean(paragraph_vectors, axis=0)
        return cosine_similarity([paragraph_vector], [frame_vector])[0][0]
    else:
        return 0

# Calculate the frame scores
df['diagnostic_score'] = df['Tokenized_Paragraph'].apply(lambda x: calculate_frame_score(x, diagnostic_vector))
df['prognostic_score'] = df['Tokenized_Paragraph'].apply(lambda x: calculate_frame_score(x, prognostic_vector))
df['identity_score'] = df['Tokenized_Paragraph'].apply(lambda x: calculate_frame_score(x, identity_vector))
df['adversarial_score'] = df['Tokenized_Paragraph'].apply(lambda x: calculate_frame_score(x, adversarial_vector))

# Check the first few rows
print(df[['Paragraph', 'diagnostic_score', 'prognostic_score', 'identity_score', 'adversarial_score']].head())

# Save the results
output_path = "C:\\Users\\User\\Desktop\\Thesis\\speeches_with_frame_scores.xlsx"
df.to_excel(output_path, index=False)
print("Results saved to:", output_path)

# Function to determine the highest scoring frame and save it in a new column
def determine_highest_frame(row):
    scores = {
        'diagnostic': row['diagnostic_score'],
        'prognostic': row['prognostic_score'],
        'identity': row['identity_score'],
        'adversarial': row['adversarial_score']
    }
    highest_frame = max(scores, key=scores.get)
    highest_score = scores[highest_frame]
    return highest_frame, highest_score

# Determine the highest scoring frame and filter based on a threshold
filtered_data = []
for index, row in df.iterrows():
    highest_frame, highest_score = determine_highest_frame(row)
    if highest_score >= 0.6:
        row['highest_frame'] = highest_frame
        filtered_data.append(row)

# Create a new DataFrame with the filtered data
filtered_df = pd.DataFrame(filtered_data)

# Check the first few rows
print(filtered_df[['Paragraph', 'diagnostic_score', 'prognostic_score', 'identity_score', 'adversarial_score', 'highest_frame']].head())

# Save the results
output_path = "C:\\Users\\User\\Desktop\\Thesis\\filtered_speeches_with_highest_frame.xlsx"
filtered_df.to_excel(output_path, index=False)
print("Results saved to:", output_path)

# Get the number of rows in the filtered dataset
num_filtered_rows = filtered_df.shape[0]
print(f"There are a total of {num_filtered_rows} rows (paragraphs) in the filtered dataset.")

# Print the column names in the filtered dataset
column_names = filtered_df.columns.tolist()
print("Column names in the filtered dataset:\n", column_names)

# Drop the 'dominant_frame' column
filtered_df = filtered_df.drop(columns=['dominant_frame'])

# Check the first few rows
print(filtered_df.head())

# Get the number of rows in the original dataset
num_original_rows = df.shape[0]
print(f"There are a total of {num_original_rows} rows (paragraphs) in the original dataset.")

# Get the number of rows in the filtered dataset
num_filtered_rows = filtered_df.shape[0]
print(f"There are a total of {num_filtered_rows} rows (paragraphs) in the filtered dataset.")

# Calculate how many rows were filtered out
num_filtered_out = num_original_rows - num_filtered_rows
print(f"A total of {num_filtered_out} rows were filtered out.")

# Save the final results
output_path = "C:\\Users\\User\\Desktop\\Thesis\\filtered_speeches_with_highest_frame.xlsx"
filtered_df.to_excel(output_path, index=False)
print("Results saved to:", output_path)